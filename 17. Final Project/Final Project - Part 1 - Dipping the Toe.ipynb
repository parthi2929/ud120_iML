{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project\n",
    "\n",
    "**Goal**:  \n",
    "To classify POI out of enron email dataset.  To achieve accuracy such that, Precision and Recall are greater than 0.3 at least.  \n",
    "\n",
    "**Premise**:  \n",
    "There are 2 important starter files, given by Udacity. \n",
    "1. poi_id.py  - This is the file we would be working on creating the classifier. \n",
    "2. tester.py  - This is simply file used to test our code  \n",
    "\n",
    "**Workflow Overview**:  \n",
    "This is just again a starter skeleton to structure our thinking process. We will add/modify/iterate as and when needed. Thus this is also not all, and complete as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Work Flow Pages: 1 -->\r\n",
       "<svg width=\"488pt\" height=\"47pt\"\r\n",
       " viewBox=\"0.00 0.00 488.00 47.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 43)\">\r\n",
       "<title>Work Flow</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-43 484,-43 484,4 -4,4\"/>\r\n",
       "<!-- step_1 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>step_1</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-38.5 97,-38.5 97,-0.5 0,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"48.5\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1. Data</text>\r\n",
       "<text text-anchor=\"middle\" x=\"48.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">PreProcessing</text>\r\n",
       "</g>\r\n",
       "<!-- step_2 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>step_2</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"133,-0.5 133,-38.5 236,-38.5 236,-0.5 133,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2. Data</text>\r\n",
       "<text text-anchor=\"middle\" x=\"184.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Standardization</text>\r\n",
       "</g>\r\n",
       "<!-- step_1&#45;&gt;step_2 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>step_1&#45;&gt;step_2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M97.1207,-19.5C105.447,-19.5 114.211,-19.5 122.833,-19.5\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122.855,-23.0001 132.855,-19.5 122.855,-16.0001 122.855,-23.0001\"/>\r\n",
       "</g>\r\n",
       "<!-- step_3 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>step_3</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"272,-0.5 272,-38.5 355,-38.5 355,-0.5 272,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"313.5\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">3. Classifier</text>\r\n",
       "<text text-anchor=\"middle\" x=\"313.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Tuning</text>\r\n",
       "</g>\r\n",
       "<!-- step_2&#45;&gt;step_3 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>step_2&#45;&gt;step_3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236.059,-19.5C244.517,-19.5 253.296,-19.5 261.759,-19.5\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"261.883,-23.0001 271.883,-19.5 261.883,-16.0001 261.883,-23.0001\"/>\r\n",
       "</g>\r\n",
       "<!-- step_4 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>step_4</title>\r\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"391,-0.5 391,-38.5 480,-38.5 480,-0.5 391,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"435.5\" y=\"-23.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">4. Evaluation</text>\r\n",
       "<text text-anchor=\"middle\" x=\"435.5\" y=\"-8.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Metrics</text>\r\n",
       "</g>\r\n",
       "<!-- step_3&#45;&gt;step_4 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>step_3&#45;&gt;step_4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M355.127,-19.5C363.248,-19.5 371.899,-19.5 380.392,-19.5\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"380.625,-23.0001 390.625,-19.5 380.625,-16.0001 380.625,-23.0001\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x44aa4a8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "g = Digraph('Work Flow',  node_attr={'shape': 'record'})\n",
    "g.attr(rankdir='LR')\n",
    "g.node('step_1', r'1. Data\\nPreProcessing')\n",
    "g.node('step_2', r'2. Data\\nStandardization')\n",
    "g.node('step_3', r'3. Classifier\\nTuning')\n",
    "g.node('step_4', r'4. Evaluation\\nMetrics')\n",
    "g.edges([('step_1','step_2'),('step_2','step_3'),('step_3','step_4')])\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Firstly let us make sure, given ```poi_id.py``` _(loaded here)_ and ```tester.py``` _(in our current directory)_ runs in current form and also to know what is the accuracy we are starting with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\parthi2929\\Anaconda3\\envs\\py2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# poi_id.py\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Example starting point. Try investigating other evaluation techniques!\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\n",
      "Accuracy: 0.25560\tPrecision: 0.18481\tRecall: 0.79800\tF1: 0.30011\tF2: 0.47968\n",
      "Total predictions: 10000\n",
      "True positives: 1596\tFalse positives: 7040\tFalse negatives:  404\tTrue negatives:  960\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tester.py\n",
    "from tester import main\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularize\n",
    "\n",
    "Recall is pretty impressive, but accuracy and precisions are pathetically low. Apparantly, Salary is not the only feature useful to identify POIs effectively. \n",
    "\n",
    "We are going to repeat the entire process of ```poi_id.py``` for evaluation almost every time we come up with new findings. So we will modularize such that, we could just call the relevant functions instead to make it more readable.Below are the modularized functions to start with.  \n",
    "\n",
    "Note we will not finish one stage (out of 4 shown above) and then go next. We will do iterative developement going back and forth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \"\"\"\n",
    "    Load the raw data_dict from Udacity's pickle data and return it\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    sys.path.append(\"../tools/\")\n",
    "    ### Load the dictionary containing the dataset\n",
    "    with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "        data_dict = pickle.load(data_file)\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "def split_data(my_dataset, features_list):\n",
    "    # convert\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    return labels, features \n",
    "\n",
    "def classify():\n",
    "    # our classifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf = GaussianNB()\n",
    "    return clf\n",
    "\n",
    "from IPython.utils.coloransi import TermColors as color # just for color gimmicks on output\n",
    "from tester import dump_classifier_and_data, main\n",
    "\n",
    "def evaluate(clf, my_dataset, features_list):\n",
    "    dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "    print '{1}Udacity\\'s Evaluation:{0}'.format(color.Normal, color.BlinkBlue)\n",
    "    return main()  # from tester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could modify ```poi_id.py``` as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5;34mUdacity's Evaluation:\u001b[0m\n",
      "GaussianNB(priors=None)\n",
      "\n",
      "Accuracy: 0.25560\tPrecision: 0.18481\tRecall: 0.79800\tF1: 0.30011\tF2: 0.47968\n",
      "Total predictions: 10000\n",
      "True positives: 1596\tFalse positives: 7040\tFalse negatives:  404\tTrue negatives:  960\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = init()\n",
    "\n",
    "\n",
    "# PRE PROCESSING\n",
    "\n",
    "\n",
    "# STANDARDIZATION\n",
    "\n",
    "\n",
    "# CLASSIFIER\n",
    "features_list = ['poi','salary']\n",
    "my_dataset = data_dict\n",
    "labels, features = split_data(my_dataset, features_list)\n",
    "clf = classify()\n",
    "\n",
    "# EVALUATION\n",
    "evaluate(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "We already have Guassian in our starter code, but how do other contenders fare? Let us try 3 more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5;34mGaussianNB:\u001b[0m\n",
      "GaussianNB(priors=None)\n",
      "\n",
      "Accuracy: 0.25560\tPrecision: 0.18481\tRecall: 0.79800\tF1: 0.30011\tF2: 0.47968\n",
      "Total predictions: 10000\n",
      "True positives: 1596\tFalse positives: 7040\tFalse negatives:  404\tTrue negatives:  960\n",
      "\n",
      "\u001b[5;34mSVC Untuned:\u001b[0m\n",
      "Total predictions: 10000\n",
      "True positives:    0\tFalse positives:   77\tFalse negatives: 2000\tTrue negatives: 7923\n",
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "\n",
      "\u001b[5;34mKMeans Untuned:\u001b[0m\n",
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "\n",
      "Accuracy: 0.77580\tPrecision: 0.20488\tRecall: 0.04200\tF1: 0.06971\tF2: 0.04994\n",
      "Total predictions: 10000\n",
      "True positives:   84\tFalse positives:  326\tFalse negatives: 1916\tTrue negatives: 7674\n",
      "\n",
      "\u001b[5;34mDecision Tree Untuned:\u001b[0m\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best')\n",
      "\n",
      "Accuracy: 0.69210\tPrecision: 0.23619\tRecall: 0.24150\tF1: 0.23881\tF2: 0.24042\n",
      "Total predictions: 10000\n",
      "True positives:  483\tFalse positives: 1562\tFalse negatives: 1517\tTrue negatives: 6438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print '{1}GaussianNB:{0}'.format(color.Normal, color.BlinkBlue)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()\n",
    "\n",
    "print '{1}SVC Untuned:{0}'.format(color.Normal, color.BlinkBlue)\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()\n",
    "\n",
    "print '{1}KMeans Untuned:{0}'.format(color.Normal, color.BlinkBlue)\n",
    "from sklearn.cluster import KMeans\n",
    "clf = KMeans(n_clusters=2)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "main()\n",
    "\n",
    "print '{1}Decision Tree Untuned:{0}'.format(color.Normal, color.BlinkBlue)\n",
    "random_state = 0\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(random_state=random_state)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)  \n",
    "_ = main()     # we made tester.py to return trained clf so to avoid re printing classifier in ipython we assign to dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SVC``` outright failed, ```KMeans``` and ```Decision Tree``` are already giving much better accuracy. Before we pick one of ```KMeans``` or ```Decision Tree```, let us check one more trap - ```Zero Prediction Accuracy. ```\n",
    "\n",
    "**What if our classifer always predicts non-POI?** Let us find out with a dummy classifier. Note we are also starting to use pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of records: 146\n",
      "Total no of POIs: 18\n",
      "Total no of POIs: 128\n",
      "Total predictions: 10000\n",
      "True positives:    0\tFalse positives:    0\tFalse negatives: 2000\tTrue negatives: 8000\n",
      "Got a divide by zero when trying out: DummyClassifier(constant=False, random_state=0, strategy='constant')\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(my_dataset, orient='index')\n",
    "\n",
    "print 'Total no of records: {}'.format(df.shape[0])\n",
    "print 'Total no of POIs: {}'.format(df.loc[df['poi'] == True].shape[0])\n",
    "print 'Total no of POIs: {}'.format(df.loc[df['poi'] == False].shape[0])\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy='constant',random_state=0, constant=False)\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "clf = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note out of 146 records, we have only 18 POIs, so intuitively, even if we always guess 'non POI' we should be right most of the times. \n",
    "\n",
    "Remember:  \n",
    "\n",
    "<div style=\"background-color:'#E3F2FD;  padding: 10px 10px 10px 10px;\"><pre style=\"background-color:'#E3F2FD;\">\n",
    "True positives (TP):    Reality POI, Prediction POI.  \n",
    "False positives (FP):   Reality non POI, Prediction POI.  \n",
    "False negatives (FN):   Reality POI, Prediction non POI.  \n",
    "True negatives (TN):    Reality non POI, Prediction non POI.</pre>  </div>\n",
    "\n",
    "Accuracy = No of predictions in reality and predicted are equal / Total no of predictions\n",
    "\n",
    "Thus\n",
    "\n",
    "$$ \\displaystyle \\text{Accuracy = }\\frac{{\\text{TP}\\text{+TN}}}{{\\text{TP+FP+FN+TN}}}=\\frac{{0+8000}}{{0+0+2000+8000}}=\\frac{{8000}}{{10000}}=80\\text{  }\\%$$  \n",
    "\n",
    "Ponder this, even if our model is simply not _predicting_ anything per se, we get 80% accuracy.  This is why, Recall and Precision are important. \n",
    "\n",
    "$$ \\displaystyle \\begin{array}{l}\\text{Precision = }\\dfrac{{\\text{TP}}}{{\\text{TP+FP}}}=\\dfrac{0}{0}\\\\\\text{Recall = }\\dfrac{{\\text{TP}}}{{\\text{TP+FN}}}=\\dfrac{0}{{2000}}=0\\end{array}$$\n",
    "\n",
    "Precision is incalculable and Recall is just 0. We are always wrongly labelling any POI that shows up in dataset as non-POI (FP) which is obviously inadmissable, and 0 Recall score indicates that.  \n",
    "\n",
    "It could be for similar reason, we cannot also currently proceed to using SVC at the moment (try checking out) \n",
    "\n",
    "Comparing Recall and Precision between, ```Decision Tree``` fares better than ```KMeans``` so we shall go with that, though accuracy is as of now lower. \n",
    "\n",
    "So far, \n",
    "1. We have set up the code \n",
    "2. Chosen Decision Tree as our main classifier to proceed with\n",
    "3. Understood, how better should our classifier fare than Zero prediction case (cross at least 80% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
